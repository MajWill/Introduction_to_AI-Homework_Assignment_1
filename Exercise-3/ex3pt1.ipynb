{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction_to_AI-Homework_Assignment_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We programmed the first part using the existing Agent class but with a Simple Reflex. To calculate the total score, executed 3 nested loops for every possible combination of starting conditions and clean/dirty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import *\n",
    "\n",
    "# Source code:\n",
    "from vacuum_world import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same `Environment` as the previous model as a template. We made some tweaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mclass\u001b[0m \u001b[0mTrivialVacuumEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"\"\"This environment has two locations, A and B. Each can be Dirty\n",
      "    or Clean. The agent perceives its location and the location's\n",
      "    status. This serves as an example of how to implement a simple\n",
      "    Environment.\"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mloc_A\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Clean\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mloc_B\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Clean\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mthing_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mWall\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mDirt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mReflexVacuumAgent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mpercept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"Returns the agent's location, and the location status (Dirty/Clean).\"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mexecute_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"Change agent's location and/or location's status; track performance.\n",
      "        Score 10 for each dirt cleaned; -1 for each move.\"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Right\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloc_B\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperformance\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Left\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloc_A\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperformance\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Suck\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Dirty\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperformance\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Clean\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource TrivialVacuumEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look to the `init` function:\n",
    "\n",
    "```py\n",
    "def __init__(self):\n",
    "    super().__init__()\n",
    "    self.status = {\n",
    "            loc_A: \"Clean\",\n",
    "            loc_B: \"Clean\"\n",
    "    }\n",
    "```\n",
    "\n",
    "Instead of using the `random` library to choose wether the room is `Clean` or `Dirty`, we initialize the state of both rooms as `Clean`: later, this will be overwritten in the `main`, where three nested *for loops* simulate all the eight possible state of the environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Initialize the two-state environment\n",
    "    trivial_vacuum_env = TrivialVacuumEnvironment()\n",
    "\n",
    "    \"\"\"We change the simpleReflexAgentProgram so that it doesn't make use of the Rule class\"\"\"\n",
    "    def SimpleReflexAgentProgram():\n",
    "        \"\"\"This agent takes action based solely on the percept. [Figure 2.10]\"\"\"\n",
    "        \n",
    "        def program(percept):\n",
    "            loc, status = percept\n",
    "            return ('Suck' if status == 'Dirty' \n",
    "                    else'Right' if loc == loc_A \n",
    "                                else'Left')\n",
    "        return program\n",
    "\n",
    "    # Create a simple reflex agent the two-state environment\n",
    "    program = SimpleReflexAgentProgram()\n",
    "    simple_reflex_agent = Agent(program)\n",
    "\n",
    "    # Dropping the agent in the environment:\n",
    "    trivial_vacuum_env.add_thing(simple_reflex_agent)\n",
    "    \n",
    "    # Initialize the score variable:\n",
    "    total_score = 0.0\n",
    "\n",
    "    # Simulating all states of the environment: \n",
    "    for p in range(2):\n",
    "        if p == 0:\n",
    "            simple_reflex_agent.location = loc_A\n",
    "        elif p == 1:\n",
    "            simple_reflex_agent.location = loc_B\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                if p == 0:\n",
    "                    simple_reflex_agent.location = loc_A\n",
    "                elif p == 1:\n",
    "                    simple_reflex_agent.location = loc_B\n",
    "                if j == 0:\n",
    "                    trivial_vacuum_env.status[loc_B] = \"Clean\"\n",
    "                else:\n",
    "                    trivial_vacuum_env.status[loc_B] = \"Dirty\"\n",
    "                if i == 0:\n",
    "                    trivial_vacuum_env.status[loc_A] = \"Clean\"\n",
    "                else:\n",
    "                    trivial_vacuum_env.status[loc_A] = \"Dirty\"\n",
    "                # Location of the agent \n",
    "                print(f\"SimpleReflexVacuumAgent is located in Room {simple_reflex_agent.location}.\\n\")\n",
    "\n",
    "                # Check the current state of the environment\n",
    "                print(f\"State of the Environment: {trivial_vacuum_env.status}.\\n\")\n",
    "                \n",
    "                # Run the environment\n",
    "                trivial_vacuum_env.step()\n",
    "\n",
    "                # Performance score after one step\n",
    "                print(f\"The score of the agent is {simple_reflex_agent.performance}.\\n\")\n",
    "                total_score = total_score + simple_reflex_agent.performance\n",
    "                simple_reflex_agent.performance = 0\n",
    "\n",
    "    # Average score\n",
    "\n",
    "    print(f\" The average score of the agent is: {total_score/8}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this snippet: \n",
    "```py\n",
    "for p in range(2):\n",
    "    if p == 0:\n",
    "        simple_reflex_agent.location = loc_A\n",
    "    elif p == 1:\n",
    "        simple_reflex_agent.location = loc_B\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            if j == 0:\n",
    "                trivial_vacuum_env.status[loc_B] = \"Clean\"\n",
    "            else:\n",
    "                trivial_vacuum_env.status[loc_B] = \"Dirty\"\n",
    "            if i == 0:\n",
    "                trivial_vacuum_env.status[loc_A] = \"Clean\"\n",
    "            else:\n",
    "                trivial_vacuum_env.status[loc_A] = \"Dirty\"\n",
    "```\n",
    "`p` simulates the position of the **agent**, `i` is the state of **Room 0** and `j` is the state of **Room 1**. The outcome will be the `8` possible combinations of position of the agent and state of the rooms.\n",
    "\n",
    "The performance then gets saved up in the `total_score` variable and the **average score** is finally calculated and printed.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In conclusion, we can state that this model as it is is not very efficient. Limited to just one action per combination, it can either **move** or **clean**, so it's performance will be greatly influenced by this problem. On the other hand, given the differences between the various possible environments and the limitation of the agent of knowing the state of the room he's standing in only, there would be the need to re-program the `step()` method to reach maximum efficiency.\n",
    "\n",
    "###### By Castagnotto Alessandro, Coceani Elisa, Majer William, Mingrone Tommaso, Secci Marco"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
